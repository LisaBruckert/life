<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of sgd</title>
  <meta name="keywords" content="sgd">
  <meta name="description" content="">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../index.html">Home</a> &gt;  <a href="index.html">compute</a> &gt; sgd.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../index.html"><img alt="<" border="0" src="../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for compute&nbsp;<img alt=">" border="0" src="../right.png"></a></td></tr></table>-->

<h1>sgd
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong></strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>function [w, R2] = sgd(y,X,numtoselect,finalstepsize,convergencecriterion,checkerror,nonneg,alpha,lambda) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre class="comment"> 
 This fucntion is working properly and can be used. But the HELP of this
 function has not been updated yet. Blame Franco. F.P. 2012
 
 Least-square stochastic gradient-descend fit.
 
 sgd(y,X,numtoselect,finalstepsize,convergencecriterion,checkerror,nonneg)
 
 &lt;y&gt; is p x 1 with the data

 &lt;X&gt; is p x q with the regressors

 &lt;numtoselect&gt; is the number of data points to randomly select on each
 iteration

 &lt;finalstepsize&gt; is like 0.05

 &lt;convergencecriterion&gt; is [A B C] where A is in (0,1), B is a positive
 integer, and C is number of percentages.
   
   We stop if we see a series of max(B,round(A*[current total-error-check
   number])) total-error-check iterations that do not improve performance
   on the estimation set, where improvement must be better by at least 1%
   of the previously marked R^2.
 
 &lt;checkerror&gt; is the number of iterations between total-error-checks

 &lt;nonneg&gt; if set to 1 costrains the solution to be positive
 
 For reference see : Kay et al. 2008 (Supplemental material)
 
 &lt;alpha, lambda&gt; ElasticNet parameters (optional, defaults to 1 and 0).
 The ElasticNet is a reguarization and variable selection algorithm. 
 The EN penalty is: 
 
 (y - X*w).^2) + lambda * sum(alpha * w.^2 + (1-alpha) * abs(w)) 

 Such that lambda sets the slope of the additional regularization error
 surface and alpha balances between the L1 and L2 constraints. When alpha
 is 1, the algorithm reduces to ridge regression. When alpha is 0, the
 algorithm reduces to the Lasso.
 
 Reference: Zou and Hastie (Zou &amp; Hastie, (2005) Journal of the Royal
 Statistical Society B 67, Part 2, pp. 301-320)
 See also: Friedman, Hastie and Tibshirani (2008). The elements of
 statistical learning, chapter 3 (page 31, equation 3.54)


 Copyright Franco Pestilli and Kendrick Kay (2013) Vistasoft Stanford University.</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../matlabicon.gif)">
</ul>
This function is called by:
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="feFitModel.html" class="code" title="function [fit w R2] = feFitModel(M,dSig,fitMethod,lambda)">feFitModel</a>	Fit the LiFE model.</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="#_sub1" class="code">function [v,len] = unitlengthfast(v,dim)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [w, R2] = sgd(y,X,numtoselect,finalstepsize,convergencecriterion,checkerror,nonneg,alpha,lambda)</a>
0002 <span class="comment">%</span>
0003 <span class="comment">% This fucntion is working properly and can be used. But the HELP of this</span>
0004 <span class="comment">% function has not been updated yet. Blame Franco. F.P. 2012</span>
0005 <span class="comment">%</span>
0006 <span class="comment">% Least-square stochastic gradient-descend fit.</span>
0007 <span class="comment">%</span>
0008 <span class="comment">% sgd(y,X,numtoselect,finalstepsize,convergencecriterion,checkerror,nonneg)</span>
0009 <span class="comment">%</span>
0010 <span class="comment">% &lt;y&gt; is p x 1 with the data</span>
0011 <span class="comment">%</span>
0012 <span class="comment">% &lt;X&gt; is p x q with the regressors</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% &lt;numtoselect&gt; is the number of data points to randomly select on each</span>
0015 <span class="comment">% iteration</span>
0016 <span class="comment">%</span>
0017 <span class="comment">% &lt;finalstepsize&gt; is like 0.05</span>
0018 <span class="comment">%</span>
0019 <span class="comment">% &lt;convergencecriterion&gt; is [A B C] where A is in (0,1), B is a positive</span>
0020 <span class="comment">% integer, and C is number of percentages.</span>
0021 <span class="comment">%</span>
0022 <span class="comment">%   We stop if we see a series of max(B,round(A*[current total-error-check</span>
0023 <span class="comment">%   number])) total-error-check iterations that do not improve performance</span>
0024 <span class="comment">%   on the estimation set, where improvement must be better by at least 1%</span>
0025 <span class="comment">%   of the previously marked R^2.</span>
0026 <span class="comment">%</span>
0027 <span class="comment">% &lt;checkerror&gt; is the number of iterations between total-error-checks</span>
0028 <span class="comment">%</span>
0029 <span class="comment">% &lt;nonneg&gt; if set to 1 costrains the solution to be positive</span>
0030 <span class="comment">%</span>
0031 <span class="comment">% For reference see : Kay et al. 2008 (Supplemental material)</span>
0032 <span class="comment">%</span>
0033 <span class="comment">% &lt;alpha, lambda&gt; ElasticNet parameters (optional, defaults to 1 and 0).</span>
0034 <span class="comment">% The ElasticNet is a reguarization and variable selection algorithm.</span>
0035 <span class="comment">% The EN penalty is:</span>
0036 <span class="comment">%</span>
0037 <span class="comment">% (y - X*w).^2) + lambda * sum(alpha * w.^2 + (1-alpha) * abs(w))</span>
0038 <span class="comment">%</span>
0039 <span class="comment">% Such that lambda sets the slope of the additional regularization error</span>
0040 <span class="comment">% surface and alpha balances between the L1 and L2 constraints. When alpha</span>
0041 <span class="comment">% is 1, the algorithm reduces to ridge regression. When alpha is 0, the</span>
0042 <span class="comment">% algorithm reduces to the Lasso.</span>
0043 <span class="comment">%</span>
0044 <span class="comment">% Reference: Zou and Hastie (Zou &amp; Hastie, (2005) Journal of the Royal</span>
0045 <span class="comment">% Statistical Society B 67, Part 2, pp. 301-320)</span>
0046 <span class="comment">% See also: Friedman, Hastie and Tibshirani (2008). The elements of</span>
0047 <span class="comment">% statistical learning, chapter 3 (page 31, equation 3.54)</span>
0048 <span class="comment">%</span>
0049 <span class="comment">%</span>
0050 <span class="comment">% Copyright Franco Pestilli and Kendrick Kay (2013) Vistasoft Stanford University.</span>
0051 
0052 <span class="comment">% Set the default for input params:</span>
0053 <span class="keyword">if</span> notDefined(<span class="string">'convergencecriterion'</span>), convergencecriterion = [.15 3 5]; <span class="keyword">end</span> 
0054 <span class="keyword">if</span> notDefined(<span class="string">'numtoselect'</span>), numtoselect = 0.1 * size(y,1); <span class="keyword">end</span>
0055 <span class="keyword">if</span> notDefined(<span class="string">'checkerror'</span>), checkerror=40; <span class="keyword">end</span>
0056 <span class="keyword">if</span> notDefined(<span class="string">'finalstepsize'</span>), finalstepsize=0.05; <span class="keyword">end</span>
0057 <span class="keyword">if</span> notDefined(<span class="string">'nonneg'</span>), nonneg = false; <span class="keyword">end</span>
0058 <span class="keyword">if</span> notDefined(<span class="string">'coordDescent'</span>), coordDescent = false; <span class="keyword">end</span>
0059 
0060 <span class="comment">% Set default values for ElasticNet (defaults to regular OLS):</span>
0061 <span class="keyword">if</span> notDefined(<span class="string">'alpha'</span>), alpha   = 0; <span class="keyword">end</span> 
0062 <span class="keyword">if</span> notDefined(<span class="string">'lambda'</span>), lambda = 0; <span class="keyword">end</span>
0063 
0064 p = size(y,1);  <span class="comment">% number of data points</span>
0065 q = size(X,2);  <span class="comment">% number of parameters</span>
0066 orig_ssq = full(sum((y).^2)); <span class="comment">% Sum of Squres fo the data</span>
0067 
0068 <span class="comment">% initialize various variables used in the fitting:</span>
0069 w          = 0 .* rand(q,1); <span class="comment">% the set of weights, between 0 and .1</span>
0070 w_best     = w;          <span class="comment">% The best set of weights.</span>
0071 est_ssq_best = inf;      <span class="comment">% minimum estimation error found so far</span>
0072 estbadcnt  = 0;          <span class="comment">% number of times estimation error has gone up</span>
0073 iter       = 1;          <span class="comment">% the iteration number</span>
0074 cnt        = 1;          <span class="comment">% the total-error-check number</span>
0075   
0076 <span class="comment">% report</span>
0077 fprintf(<span class="string">'[%s] Performing fit | %d measurements | %d parameters | '</span>,mfilename,p,q);
0078 
0079 <span class="comment">% Start computing the fit.</span>
0080 <span class="keyword">while</span> 1
0081   <span class="comment">% Indices to selected signal and model</span>
0082   ix      = randi(p,1,numtoselect); <span class="comment">% Slower indexing method</span>
0083   ix2     = false(p,1);
0084   ix2(ix) = true;
0085 
0086   <span class="comment">% select the subset of signal and model to use for fitting</span>
0087   y0 = y(ix2);
0088   X0 = X(ix2,:);
0089   
0090   <span class="comment">% if not the first iteration, adjust parameters</span>
0091   <span class="keyword">if</span> iter ~= 1
0092     <span class="comment">% Calculate the gradient (change in error over change in parameter):</span>
0093     grad = -((y0 - X0*w)' * X0)' + lambda * (alpha + 2*(1 - alpha)*w);
0094     
0095     <span class="comment">% This computes the coordinate descent instead of the gradient descent.</span>
0096     <span class="keyword">if</span> coordDescent
0097         <span class="comment">% Coordinate descent</span>
0098         m    = min(grad);
0099         grad = (grad==m)*m;
0100     <span class="keyword">end</span>
0101     
0102     <span class="comment">% Unit-length normalize the gradient</span>
0103     grad = <a href="#_sub1" class="code" title="subfunction [v,len] = unitlengthfast(v,dim)">unitlengthfast</a>(grad);
0104     
0105     <span class="comment">% Perform gradient descent</span>
0106     w = w - finalstepsize*grad;
0107         
0108     <span class="comment">% Non-negative constrain, we set negative weights to zero</span>
0109     <span class="keyword">if</span> ( nonneg ), w(w&lt;0) = 0;<span class="keyword">end</span>
0110   <span class="keyword">end</span>
0111   
0112   <span class="comment">% check the total error every so often</span>
0113   <span class="keyword">if</span> mod(iter,checkerror) == 1
0114     <span class="comment">% Curent estimated sum of the squares of the residuals (SSQ)</span>
0115     est_ssq = sum((y - X*w).^2) + lambda*(alpha*sum(w) + (1-alpha)*sum(w.^2));
0116      
0117     <span class="comment">% Check if the SSQ improved</span>
0118     isimprove = est_ssq &lt; est_ssq_best; 
0119             
0120     <span class="comment">% We keep fitting if the SSQ is not Inf OR some percent smaller than</span>
0121     <span class="comment">% the best SSQ obtained so far</span>
0122     <span class="comment">%keepfitting = isinf(est_ssq_best) | (est_ssq &lt; ((est_ssq_best - min_ssq)));</span>
0123     keepfitting = isinf(est_ssq_best) | (est_ssq &lt; ((est_ssq_best * (1-convergencecriterion(3)/100))));
0124 
0125     <span class="comment">% do we consider this iteration to be the best yet?</span>
0126     <span class="keyword">if</span> isimprove
0127       <span class="comment">% The SSQ was smaller, the fit improved.</span>
0128       w_best       = w;       <span class="comment">% Set the current to be the best so far</span>
0129       est_ssq_best = est_ssq; <span class="comment">% The min error</span>
0130       
0131       <span class="comment">% OK we improved, but check whether improvement is too small to be</span>
0132       <span class="comment">% considered useful.</span>
0133       <span class="keyword">if</span> keepfitting
0134         <span class="comment">% THe fit improved more than the minimum accptable improvement.</span>
0135         <span class="comment">% Reset the counter fo rthe bad fits, so that we start over</span>
0136         <span class="comment">% checking for stopping.</span>
0137         estbadcnt  = 0;
0138         <span class="comment">%est_ssq_best = est_ssq; %</span>
0139       <span class="keyword">else</span>
0140         estbadcnt = estbadcnt + 1;
0141       <span class="keyword">end</span>
0142     <span class="keyword">else</span>
0143       <span class="comment">% The fit actually was bad, SSQ increases count how many bad fit we had. Stop after a centrain number.</span>
0144       estbadcnt = estbadcnt + 1;
0145     <span class="keyword">end</span>
0146     
0147     <span class="comment">% stop if we haven't improved in a while</span>
0148     <span class="keyword">if</span> estbadcnt &gt;= max(convergencecriterion(2),round(convergencecriterion(1)*cnt))
0149       R2 = 100*(1-(est_ssq_best/orig_ssq));
0150       fprintf(<span class="string">' DONE fitting | SSQ=%2.3f (Original SSQ=%2.3f) | Rzero-squared %2.3f%%.\n'</span>,<span class="keyword">...</span>
0151               est_ssq_best,orig_ssq,R2);
0152       <span class="keyword">break</span>;
0153     <span class="keyword">end</span>
0154     
0155     <span class="comment">% Update the counter</span>
0156     cnt = cnt + 1;
0157   <span class="keyword">end</span>
0158   iter = iter + 1;
0159 <span class="keyword">end</span>
0160 
0161 <span class="comment">% prepare output</span>
0162 w = w_best;
0163 
0164 <a name="_sub1" href="#_subfunctions" class="code">function [v,len] = unitlengthfast(v,dim)</a>
0165 
0166 <span class="comment">% function [v,len] = unitlengthfast(v,dim)</span>
0167 <span class="comment">%</span>
0168 <span class="comment">% &lt;v&gt; is a vector (row or column) or a 2D matrix</span>
0169 <span class="comment">% &lt;dim&gt; (optional) is dimension along which vectors are oriented.</span>
0170 <span class="comment">%   if not supplied, assume that &lt;v&gt; is a row or column vector.</span>
0171 <span class="comment">%</span>
0172 <span class="comment">% unit-length normalize &lt;v&gt;.  aside from input flexibility,</span>
0173 <span class="comment">% the difference between this function and unitlength.m is that</span>
0174 <span class="comment">% we do not deal with NaNs (i.e. we assume &lt;v&gt; does not have NaNs),</span>
0175 <span class="comment">% and if a vector has 0 length, it becomes all NaNs.</span>
0176 <span class="comment">%</span>
0177 <span class="comment">% we also return &lt;len&gt; which is the original vector length of &lt;v&gt;.</span>
0178 <span class="comment">% when &lt;dim&gt; is not supplied, &lt;len&gt; is a scalar.  when &lt;dim&gt; is</span>
0179 <span class="comment">% supplied, &lt;len&gt; is the same dimensions as &lt;v&gt; except collapsed</span>
0180 <span class="comment">% along &lt;dim&gt;.</span>
0181 <span class="comment">%</span>
0182 <span class="comment">% note some weird cases:</span>
0183 <span class="comment">%   unitlengthfast([]) is [].</span>
0184 <span class="comment">%   unitlengthfast([0 0]) is [NaN NaN].</span>
0185 <span class="comment">%</span>
0186 <span class="comment">% example:</span>
0187 <span class="comment">% a = [3 0];</span>
0188 <span class="comment">% isequalwithequalnans(unitlengthfast(a),[1 0])</span>
0189 
0190 <span class="keyword">if</span> nargin==1
0191   len = sqrt(v(:).'*v(:));
0192   v = v / len;
0193 <span class="keyword">else</span>
0194   <span class="keyword">if</span> dim==1
0195     len = sqrt(sum(v.^2,1));
0196     v = v ./ repmat(len,[size(v,1) 1]);  <span class="comment">% like this for speed.  maybe use the indexing trick to speed up even more??</span>
0197   <span class="keyword">else</span>
0198     len = sqrt(sum(v.^2,2));
0199     v = v ./ repmat(len,[1 size(v,2)]);
0200   <span class="keyword">end</span>
0201 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Tue 01-Jul-2014 13:06:36 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>